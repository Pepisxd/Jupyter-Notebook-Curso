{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3e4282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import nbformat\n",
    "from nbformat.v4 import new_notebook, new_markdown_cell, new_code_cell\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd949c77",
   "metadata": {},
   "source": [
    "# Cargamos las bibliotecas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6258dcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leer_notebook(ruta):\n",
    "    with open(ruta, 'r', encoding='utf-8') as f:\n",
    "        return nbformat.read(f, as_version=4)\n",
    "    \n",
    "def guardar_notebook(notebook, ruta):\n",
    "    with open(ruta, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(notebook, f)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1922e22b",
   "metadata": {},
   "source": [
    "## Procesador de notebooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a5ed0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneradorEducativo:\n",
    "    def __init__(self):\n",
    "        self.generador_ejercicios = pipeline(\n",
    "            'text-generation', \n",
    "            model='google/flan-t5-large'\n",
    "        )\n",
    "        self.generador_resumen = pipeline(\n",
    "            'summarization',\n",
    "            model='facebook/bart-large-cnn'\n",
    "        )\n",
    "    \n",
    "    def crear_ejercicio(self, tema):\n",
    "        prompt = f\"\"\"Crea un ejercicio pr√°ctico sobre {tema} para estudiantes con:\n",
    "        - Enunciado claro\n",
    "        - Respuesta oculta tras un bot√≥n\n",
    "        - Dificultad media\"\"\"\n",
    "        \n",
    "        ejercicio = self.generador_ejercicios(\n",
    "            prompt,\n",
    "            max_length=200,\n",
    "            temperature=0.7\n",
    "        )[0]['generated_text']\n",
    "        \n",
    "        return self._formatear_ejercicio(ejercicio)\n",
    "    \n",
    "    def _formatear_ejercicio(self, texto):\n",
    "        # Widget interactivo\n",
    "        boton = widgets.ToggleButton(\n",
    "            value=False,\n",
    "            description='Mostrar respuesta',\n",
    "            button_style=''\n",
    "        )\n",
    "        salida = widgets.Output()\n",
    "        \n",
    "        def on_click(change):\n",
    "            with salida:\n",
    "                if change['new']:\n",
    "                    print(\"üîç Respuesta:\")\n",
    "                    print(texto.split(\"Respuesta:\")[1] if \"Respuesta:\" in texto else texto)\n",
    "                else:\n",
    "                    salida.clear_output()\n",
    "        \n",
    "        boton.observe(on_click, names='value')\n",
    "        \n",
    "        display(widgets.VBox([\n",
    "            widgets.HTML(value=f\"<h3>üìù Ejercicio</h3><p>{texto.split('Respuesta:')[0]}</p>\"),\n",
    "            boton,\n",
    "            salida\n",
    "        ]))\n",
    "    \n",
    "    def crear_resumen(self, contenido):\n",
    "        resumen = self.generador_resumen(\n",
    "            contenido,\n",
    "            max_length=150,\n",
    "            min_length=30\n",
    "        )[0]['summary_text']\n",
    "        \n",
    "        display(widgets.HTML(\n",
    "            value=f\"<div style='background:#f8f9fa;padding:15px;border-radius:5px'>\"\n",
    "                 f\"<h3>üìö Resumen</h3><p>{resumen}</p></div>\"\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdab128d",
   "metadata": {},
   "source": [
    "## Generador de contenido educativo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6517d371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8f1d6ae4624962a71952a28c082409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2047276cc1da4da09e80f5492ccc9d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93162ca7b3754802a2fa586402e22606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb5d88a1b3dc42cea81d4f079114083e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ba1c4143524fb8862a40fbd1963e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41b4b9f0e9d46beb612a63068eebcfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e59089b4ce411b8e6596000f746085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['PeftModelForCausalLM', 'AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BitNetForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b160f5ea81c94cbd94ac948eb69162b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2c725f332c429ea74f4c7dbc0e43ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcfbeac95b5a44d993e2a2652813ce9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d9f3f4c6e3a467ab727b7aeb537091b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d415537dee743929f3b55ccf0c86f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79cbfd830fd540d7be6df13706bcce17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Your max_length is set to 150, but your input_length is only 25. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83d4fb3eebb438f9e6eccd3676d3dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value=\"<div style='background:#f8f9fa;padding:15px;border-radius:5px'><h3>üìö Resumen</h3><p>Las redes neur‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4691f9a06ea54d778d165bb79eee6f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>üìù Ejercicio</h3><p>Crea un ejercicio pr√°ctico sobre redes neuronales para estud‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inicializar\n",
    "generador = GeneradorEducativo()\n",
    "\n",
    "# Ejemplo de uso\n",
    "contenido = \"\"\"\n",
    "Las redes neuronales son modelos computacionales inspirados en el cerebro humano...\n",
    "\"\"\"\n",
    "generador.crear_resumen(contenido)\n",
    "\n",
    "# Generar ejercicio\n",
    "generador.crear_ejercicio(\"redes neuronales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a66993e",
   "metadata": {},
   "source": [
    "## Uso en el notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c21417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enriquecer_notebook(ruta_entrada, ruta_salida):\n",
    "    notebook = leer_notebook(ruta_entrada)\n",
    "    generador = GeneradorEducativo()\n",
    "    \n",
    "    # Extraer texto de celdas markdown\n",
    "    contenido = \"\\n\".join(\n",
    "        cell['source'] for cell in notebook.cells if cell.cell_type == 'markdown'\n",
    "    )\n",
    "    \n",
    "    # A√±adir resumen al inicio\n",
    "    resumen = generador.crear_resumen(contenido)\n",
    "    notebook.cells.insert(0, new_markdown_cell(\n",
    "        \"## Resumen Autom√°tico\\n\"\n",
    "    ))\n",
    "\n",
    "    # A√±adir ejercicios cada 3 celdas\n",
    "    for idx in range(0, len(notebook.cells), 3):\n",
    "        cell = notebook.cells[idx]\n",
    "        if cell.cell_type == 'markdown':\n",
    "            tema = cell['source'].split()[0] if cell['source'].split() else \"Tema\"\n",
    "            ejercicio = generador.crear_ejercicio(tema)\n",
    "            notebook.cells.insert(\n",
    "                idx + 1,\n",
    "                new_markdown_cell(\n",
    "                    f\"## Ejercicio Autogenerado\\n\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "    guardar_notebook(notebook, ruta_salida)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e867e6ba",
   "metadata": {},
   "source": [
    "## Procesar Notebooks existentes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96049520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Notebook enriquecido con ejercicios y resumen!\n"
     ]
    }
   ],
   "source": [
    "# Procesar un notebook de ejemplo\n",
    "#enriquecer_notebook(\"temas.ipynb\", \"temas_con_ejercicios.ipynb\")\n",
    "\n",
    "print(\"‚úÖ Notebook enriquecido con ejercicios y resumen!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia_jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
